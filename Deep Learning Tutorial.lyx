#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass "I:/Program/LyX 2.2/Resources/layouts/llncs"
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Deep Learning Tutorial
\end_layout

\begin_layout Author
DJN_DL
\end_layout

\begin_layout Section
Introduction to Deep Learning
\end_layout

\begin_layout Subsection
Framework
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Step.1 A set of functions 
\begin_inset Formula $f_{1},f_{2},...$
\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Step.2 Feed training data to determine goodness of the functions
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Step.3 Pick the best function 
\begin_inset Formula $f^{*}$
\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Step.4 Use 
\begin_inset Formula $f^{*}$
\end_inset

to test
\end_layout

\begin_layout Subsection
Neural Network
\end_layout

\begin_layout Description

\emph on
Neuron 
\begin_inset Formula $z=a_{1}w_{1}+\cdots+a_{k}w_{k}+\cdots+a_{K}w_{K}+b$
\end_inset


\end_layout

\begin_layout Description
\begin_inset Graphics
	filename pasted1.png
	scale 80
	clip

\end_inset


\end_layout

\begin_layout Description
Sigmoid
\begin_inset space \space{}
\end_inset

Function 
\begin_inset Formula $\sigma(z)=\dfrac{1}{1+e^{-z}}$
\end_inset


\end_layout

\begin_layout Standard
Different connetctions leads to different network structure.
 Each neurons can have different values of weights and biases.
 
\emph on
Weights
\emph default
 and 
\emph on
biases
\emph default
 are network parameters 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
Network example: Fully connect feedforward network
\end_layout

\begin_layout Standard
Given parameters 
\begin_inset Formula $\theta$
\end_inset

define a function, a given network structure defines a function set.
\end_layout

\begin_layout Standard
In general, the output of a network can be arbitrary.
 
\begin_inset CommandInset href
LatexCommand href
name "Softmax function"
target "https://en.wikipedia.org/wiki/Softmax_function"

\end_inset

: in probability theory, the output of the softmax function can be used
 to represent a 
\emph on
categorical distribution
\emph default
 - that is, a probability distribution over 
\begin_inset Formula $K$
\end_inset

 different possible outcomes.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png

\end_inset


\end_layout

\begin_layout Standard
Total loss 
\begin_inset Formula $L=\sum_{r=1}^{R}l_{r}$
\end_inset

 is expected to be as small as possible.
 Finding a function in a function set that minimizes the total loss 
\begin_inset Formula $L$
\end_inset

 means finding the network parameters 
\begin_inset Formula $\theta^{*}$
\end_inset

that minimize total loss 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Subparagraph*
How do we find these parameters
\end_layout

\begin_layout Standard

\emph on
Gradient descent
\emph default
 
\end_layout

\begin_layout Standard

\emph on
Backpropagation
\emph default
 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted3.png
	scale 50

\end_inset


\end_layout

\begin_layout Section
Why Deep?
\end_layout

\begin_layout Standard
The deeper the better.
\end_layout

\begin_layout Subparagraph*
Universality Theorem 
\end_layout

\begin_layout Theorem
Any continous function 
\begin_inset Formula $f:R^{N}\rightarrow R^{M}$
\end_inset

can be realized by a network with one hidden layer, given enough hidden
 neurons.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
And actually 
\begin_inset Quotes eld
\end_inset

deepness
\begin_inset Quotes erd
\end_inset

 performs better than one-layer 
\begin_inset Quotes eld
\end_inset

fatness
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted4.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
The modularzation is automatically learned from data.
\end_layout

\begin_layout Description
Toolkit Keras
\end_layout

\end_body
\end_document
